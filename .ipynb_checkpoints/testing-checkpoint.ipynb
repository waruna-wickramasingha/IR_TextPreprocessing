{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'the': 20, 'good': 13, 'and': 13, 'br': 11, 'to': 10, 'lectures': 9, 'i': 9, 'very': 8, 'in': 8, 'well': 7, 'were': 7, 'are': 6, 'it': 6, 'a': 6, 'can': 6, 'examples': 6, 'slides': 5, 'is': 5, 'class': 5, 'more': 5, 'lecture': 4, 'also': 4, 'questions': 4, '039': 4, 'we': 4, 'understand': 4, 'was': 4, 'about': 4, 'with': 4, 'teaching': 3, 'you': 3, 'organized': 3, 'of': 3, 'oop': 3, 'if': 3, 'codes': 3, 'concepts': 3, 'clear': 3, 'understandable': 2, 'given': 2, 'ask': 2, 'from': 2, 'please': 2, 'do': 2, 'at': 2, 's': 2, 'better': 2, 'for': 2, 'us': 2, 'sometimes': 2, 'speed': 2, 'take': 2, 'clearly': 2, 'will': 2, 'principles': 2, 'lot': 2, 'learn': 2, 'would': 2, 'coding': 2, 'think': 2, 'board': 2, 'my': 2, 'that': 2, 'be': 2, 'explanations': 2, 'too': 2, 't': 2, 'honestly': 1, 'last': 1, 'seven': 1, 'useful': 1, 'self': 1, 'study': 1, 'opportunity': 1, 'lecturer': 1, 'appreciative': 1, 'recap': 1, 'starting': 1, 'high': 1, 'thanks': 1, 'but': 1, 'bit': 1, 'working': 1, 'activity': 1, 'must': 1, 'one': 1, 'so': 1, 'another': 1, 'hour': 1, 'thursdays': 1, 'madame': 1, 'hear': 1, 'your': 1, 'voice': 1, 'things': 1, 'teach': 1, 'presentation': 1, 'source': 1, 'refer': 1, 'lf': 1, 'example': 1, 'within': 1, 'classroom': 1, 'help': 1, 'structured': 1, 'easy': 1, 'labs': 1, 'done': 1, 'helped': 1, 'this': 1, 'new': 1, 'language': 1, 'motivated': 1, 'have': 1, 'been': 1, 'discussed': 1, 'solutions': 1, 'exercisers': 1, 'learned': 1, 'write': 1, 'when': 1, 'compare': 1, 'yours': 1, 'mistakes': 1, 'practices': 1, 'should': 1, 'follow': 1, 'there': 1, 'fore': 1, 'great': 1, 'discuss': 1, 'madam': 1, 'explained': 1, 'interesting': 1, 'want': 1, 'scenario': 1, 'answers': 1, 'future': 1, 'satisfy': 1, 'first': 1, '7': 1, 'way': 1, 'really': 1, 'coming': 1, 'lectuers': 1, 'effort': 1, 'make': 1, 'undersatand': 1, 'every': 1, 'student': 1, 'room': 1, 'helpfull': 1, 'able': 1, 'obtain': 1, 'picture': 1, 'its': 1, 'letting': 1, 'explain': 1, 'again': 1, 'suitable': 1, 'some': 1, 'on': 1, 'white': 1, 'unclear': 1, 'back': 1, 'overall': 1, 'they': 1, 'weren': 1, 'fast': 1, 'writing': 1, 'code': 1, 'somewhat': 1, 'confusing': 1, 'because': 1, 'didn': 1, 'know': 1, 'java': 1, 'before': 1, 'actually': 1, 'easily': 1, 'by': 1, 'which': 1, 'helpful': 1, 'provide': 1, 'solved': 1, 'as': 1, 'thankyou': 1})\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def words(text): \n",
    "    return re.findall(r'\\w+', text.lower())\n",
    "    #return text.split()\n",
    "\n",
    "WORDS = Counter(words(open('student_cource_feedback.txt').read()))\n",
    "#WORDS = Counter(words(open('test.txt').read()))\n",
    "\n",
    "print(WORDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from symspellpy.symspellpy import SymSpell  # import the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "string\n",
      ",\n",
      "and-hypens-and\n",
      "a\n",
      "cat\n",
      "#\n",
      "wow\n",
      ",\n",
      "@\n",
      "john\n",
      "a.\n",
      "Kelllo\n",
      "words\n"
     ]
    }
   ],
   "source": [
    "#from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "my_sent = open('test.txt').read()\n",
    "tokens = word_tokenize(my_sent)\n",
    "#tokens = sent_tokenize(my_sent)\n",
    "\n",
    "for item in tokens:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something\n",
      "{'something'}\n",
      "happening\n",
      "{'happening', 'penning', 'henning'}\n"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "# find those words that may be misspelled\n",
    "misspelled = spell.unknown(['somsething', 'is', 'hapenning', 'here'])\n",
    "\n",
    "for word in misspelled:\n",
    "    # Get the one `most likely` answer\n",
    "    print(spell.correction(word))\n",
    "\n",
    "    # Get a list of `likely` options\n",
    "    print(spell.candidates(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from symspellpy.symspellpy import SymSpell, Verbosity  # import the module\n",
    "\n",
    "# maximum edit distance per dictionary precalculation\n",
    "max_edit_distance_dictionary = 2\n",
    "prefix_length = 7\n",
    "# create object\n",
    "\n",
    "sym_spell = SymSpell(max_edit_distance_dictionary, prefix_length)\n",
    "\n",
    "# create dictionary using big.txt\n",
    "if not sym_spell.create_dictionary(\"big.txt\"):\n",
    "    print(\"Corpus file not found\")\n",
    "\n",
    "#for key, count in sym_spell.words.items():\n",
    "#    print(\"{} {}\".format(key, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing spell correction, 3, -33.521509978693395\n"
     ]
    }
   ],
   "source": [
    "input_term = \"testingspellcorrec tion\"\n",
    "    \n",
    "result = sym_spell.word_segmentation(input_term)\n",
    "\n",
    "print(\"{}, {}, {}\".format(result.corrected_string, result.distance_sum,result.log_prob_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "str = open(\"twitter_feed.txt\", mode='r', encoding=\"Latin-1\").read()\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, regexp_tokenize\n",
    "\n",
    "tokens = word_tokenize(str)\n",
    "\n",
    "f = open(\"Tokens_twitter_feed.out\", mode='w+', encoding=\"Latin-1\")\n",
    "for token in tokens:\n",
    "    f.write(token + \"\\n\")\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
